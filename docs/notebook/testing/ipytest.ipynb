{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ipytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file name (required)\n",
    "__file__ = \"testing.ipynb\"\n",
    "\n",
    "# Add ipython magics\n",
    "# Add ipython magics\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "def test_sorted():\n",
    "    assert sorted([4, 2, 1, 3]) == [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.fixture\n",
    "def dict_list():\n",
    "    return [\n",
    "        dict(a='a', b=3),\n",
    "        dict(a='c', b=1),\n",
    "        dict(a='b', b=2),\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_sorted__key_example_1(dict_list):\n",
    "    assert sorted(dict_list, key=lambda d: d['a']) == [\n",
    "        dict(a='a', b=3),\n",
    "        dict(a='b', b=2),\n",
    "        dict(a='c', b=1),\n",
    "    ]\n",
    "\n",
    "\n",
    "def test_sorted__key_example_2(dict_list):\n",
    "    assert sorted(dict_list, key=lambda d: d['b']) == [\n",
    "        dict(a='c', b=1),\n",
    "        dict(a='b', b=2),\n",
    "        dict(a='a', b=3),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test parameterisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                           [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.00s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "@pytest.mark.parametrize('input,expected', [\n",
    "    ([2, 1], [1, 2]),\n",
    "    ('zasdqw', list('adqswz')),\n",
    "])\n",
    "def test_examples(input, expected):\n",
    "    actual = sorted(input)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "### `%%run_pytest â€¦`\n",
    "\n",
    "IPython magic that executes first the cell and then `run_pytest`. Arguments passed in the cell are passed directly to pytest. The Magics should have been imported with `import ipytest.magics` beforehand. \n",
    "\n",
    "### `ipytest.run_pytest(module=None, filename=None, pytest_options=(), pytest_plugins=())`\n",
    "\n",
    "runs the tests in the existing module (by default `main`) with pytest.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "* `module`: the module that contains the tests. If not specified, `__main__` is used.\n",
    "* `filename`: Filename of the file containing the tests. If nothing is specified, the `__file__`attribute of the passed module is used.\n",
    "* `pytest_options`: additional options passed to pytest.\n",
    "* `pytest_plugins`: additional pytest plugins.\n",
    "\n",
    "### `ipytest.run_tests(doctest=False, items=None)`\n",
    "\n",
    "Arguments:\n",
    "\n",
    "* `doctest`: If `True` is specified, angegeben wird, then doctests are searched for.\n",
    "* `items`: The *globals* object that contains the tests. If `None` is specified, the *globals* object is obtained from the call stack.\n",
    "\n",
    "### `ipytest.clean_tests(pattern=\"test*\", items=None)`\n",
    "\n",
    "deletes those tests whose names match the specified pattern.\n",
    "\n",
    "In IPython, the results of all evaluations are saved in global variables, unless they are explicitly deleted. This behavior implies that if tests are renamed, the previous definitions will still be found if they are not deleted. This method aims to simplify this process.\n",
    "\n",
    "An effective method is `clean_tests` to start with a cell, then define all test cases and finally `run_tests` call them. That way, renaming tests works as expected.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "* `pattern`: A glob pattern that is used to find the tests to delete.\n",
    "* `items`: The *globals* object that contains the tests. If `None` is specified, the globals object is obtained from the call stack.\n",
    "\n",
    "### `ipytest.collect_tests(doctest=False, items=None)`\n",
    "\n",
    "collects all test cases and sends them to `unittest.TestSuite`.\n",
    "\n",
    "The arguments are the same as for `ipytest.run_tests`.\n",
    "\n",
    "### `ipytest.assert_equals(a, b, *args, **kwargs)`\n",
    "\n",
    "compares two objects and throws an exception if they are not the same.\n",
    "\n",
    "The method `ipytest.get_assert_function` determines the assert implementation to be used depending on the following arguments:\n",
    "\n",
    "* `a, b`: the two objects to be compared.\n",
    "* `args, kwargs`: (Keyword) arguments that are passed to the underlying test function.\n",
    "\n",
    "### `ipytest.get_assert_function(a, b)`\n",
    "\n",
    "determines the assert function to be used depending on the arguments.\n",
    "\n",
    "If one of the objects is `numpy.ndarray`, `pandas.Series`, `pandas.DataFrame` or `pandas.Panel` the assert functions provided by `numpy` and `pandas` will be returned.\n",
    "\n",
    "### `ipytest.unittest_assert_equals(a, b)`\n",
    "\n",
    "compares two objects using the `assertEqual` method of `unittest.TestCase`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 Kernel",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
